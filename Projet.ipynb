{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Collecte de donn√©es\n",
    "\n",
    "Vous devez collecter et t√©l√©charger un ensemble d'images. Vous avez les t√¢ches suivantes √† programmer, en automatisant le processus autant que possible :\n",
    "\n",
    "1.  Cr√©er un dossier appel√© *images*.\n",
    "2.  T√©l√©charger les images sous licence ouverte dans le dossier *images* (minimum 100\n",
    "    images).\n",
    "3.  Enregistrez les m√©tadonn√©es de chaque image comme la taille de l'image, le format de l'image (.jpeg,\n",
    "    .png, etc.), l'orientation de l'image (paysage, portrait, carr√©, etc.),\n",
    "    date de cr√©ation, mod√®le d'appareil photo, etc. dans un ou plusieurs fichiers JSON. Vous pouvez utiliser les informations [Exif](https://en.wikipedia.org/wiki/Exif) pr√©sentes dans les fichiers d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Sp√©cifiez le chemin du dossier que vous souhaitez cr√©er\n",
    "dossier = 'images'\n",
    "\n",
    "# Cr√©ez le dossier\n",
    "os.makedirs(dossier, exist_ok=True)\n",
    "\n",
    "print(f\"Le dossier '{dossier}' a √©t√© cr√©√© avec succ√®s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialiser le wrapper SPARQL\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "IMAGE_DIR = \"images\"  # Dossier de sauvegarde des images\n",
    "\n",
    "# D√©finir la requ√™te SPARQL pour obtenir des images de monuments\n",
    "sparql.setQuery(\"\"\"\n",
    "SELECT ?monument ?monumentLabel ?image WHERE {\n",
    "  ?monument wdt:P31 wd:Q839954.  # Monument\n",
    "  ?monument wdt:P18 ?image.      # Image\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "try:\n",
    "    # Ex√©cuter la requ√™te et r√©cup√©rer les r√©sultats en JSON\n",
    "    results = sparql.query().convert()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'ex√©cution de la requ√™te SPARQL : {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Liste des images √† t√©l√©charger avec des indices de 1 √† 100\n",
    "images = [(i + 1, result[\"image\"][\"value\"]) for i, result in enumerate(results[\"results\"][\"bindings\"])]\n",
    "\n",
    "# D√©finition d'un User-Agent personnalis√©\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"MonScript/1.0 (mailto:mohamedguef@gmail.com)\"  # Remplace par ton email pour respecter la politique\n",
    "}\n",
    "\n",
    "def download_image(data):\n",
    "    index, image_url = data\n",
    "    try:\n",
    "        # Nom fixe sous le format image_1.jpg, image_2.jpg, ..., image_100.jpg\n",
    "        image_name = f\"image_{index}.jpg\"\n",
    "        image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "\n",
    "        # T√©l√©charger l'image avec un User-Agent correct\n",
    "        response = requests.get(image_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()  # V√©rifie si la requ√™te a r√©ussi\n",
    "        \n",
    "        # Sauvegarder l'image\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"‚úÖ T√©l√©charg√© : {image_name}\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du t√©l√©chargement de {image_url} : {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur inattendue : {e}\")\n",
    "\n",
    "# T√©l√©chargement des images en parall√®le\n",
    "MAX_THREADS = 10  # Ajuste ce nombre en fonction de ta connexion et des performances\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    executor.map(download_image, images)\n",
    "\n",
    "print(\"‚úÖ Tous les t√©l√©chargements sont termin√©s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Pillow ExifRead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import exifread\n",
    "from tqdm import tqdm  # Barre de progression\n",
    "\n",
    "# D√©finir le dossier des images et le fichier de sortie JSON\n",
    "IMAGE_DIR = \"images\"\n",
    "METADATA_FILE = os.path.join(IMAGE_DIR, \"metadata.json\")\n",
    "\n",
    "# V√©rifier que le dossier \"images\" existe\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    print(f\"‚ö†Ô∏è Dossier '{IMAGE_DIR}' introuvable. V√©rifiez que les images sont bien t√©l√©charg√©es.\")\n",
    "    exit(1)\n",
    "\n",
    "# Liste pour stocker les m√©tadonn√©es de toutes les images\n",
    "metadata_list = []\n",
    "\n",
    "# Liste des fichiers images tri√©s\n",
    "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "# Parcourir les images et extraire les m√©tadonn√©es\n",
    "for image_name in tqdm(image_files, desc=\"üì∑ Extraction des m√©tadonn√©es\"):\n",
    "    image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "\n",
    "    try:\n",
    "        # V√©rifier si l'image est corrompue en tentant de l'ouvrir\n",
    "        with Image.open(image_path) as img:\n",
    "            img.verify()  # V√©rifie l'int√©grit√© de l'image sans la charger\n",
    "            img = Image.open(image_path)  # Recharge l'image pour la lecture des m√©tadonn√©es\n",
    "            width, height = img.size\n",
    "            format_ = img.format\n",
    "            orientation = 'Portrait' if height > width else 'Paysage' if width > height else 'Carr√©'\n",
    "\n",
    "        # V√©rifier si c'est un PNG (pas d'Exif)\n",
    "        if format_.upper() == \"PNG\":\n",
    "            exif_data = \"Non disponible (format PNG)\"\n",
    "        else:\n",
    "            # Extraire les m√©tadonn√©es Exif avec exifread\n",
    "            with open(image_path, 'rb') as f:\n",
    "                tags = exifread.process_file(f, stop_tag=\"EXIF DateTimeOriginal\", details=False)\n",
    "\n",
    "            # Filtrer les champs probl√©matiques\n",
    "            exif_data = {\n",
    "                \"Date de creation\": str(tags.get(\"EXIF DateTimeOriginal\", \"Inconnu\")),\n",
    "                \"Modele d'appareil\": str(tags.get(\"Image Model\", \"Inconnu\")),\n",
    "                \"Marque d'appareil\": str(tags.get(\"Image Make\", \"Inconnu\")),\n",
    "                \"ISO\": str(tags.get(\"EXIF ISOSpeedRatings\", \"Inconnu\")),\n",
    "                \"Temps d'exposition\": str(tags.get(\"EXIF ExposureTime\", \"Inconnu\")),\n",
    "                \"Ouverture (f)\": str(tags.get(\"EXIF FNumber\", \"Inconnu\")),\n",
    "                \"Longueur focale\": str(tags.get(\"EXIF FocalLength\", \"Inconnu\")),\n",
    "                \"GPS Latitude\": str(tags.get(\"GPS GPSLatitude\", \"Non disponible\")),\n",
    "                \"GPS Longitude\": str(tags.get(\"GPS GPSLongitude\", \"Non disponible\")),\n",
    "            }\n",
    "\n",
    "            # Supprimer les champs corrompus (√©vite les erreurs \"Possibly corrupted field\")\n",
    "            exif_data = {key: value for key, value in exif_data.items() if \"Possibly corrupted\" not in value}\n",
    "\n",
    "        # Ajouter les m√©tadonn√©es √† la liste\n",
    "        metadata_list.append({\n",
    "            'Nom du fichier': image_name,\n",
    "            'Taille': {'Largeur': width, 'Hauteur': height},\n",
    "            'Format': format_,\n",
    "            'Orientation': orientation,\n",
    "            'Exif': exif_data\n",
    "        })\n",
    "\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"‚ö†Ô∏è Image illisible : {image_name} (corrompue ou format inconnu)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur inattendue sur {image_name} : {e}\")\n",
    "\n",
    "# Enregistrer toutes les m√©tadonn√©es dans un fichier JSON\n",
    "try:\n",
    "    with open(METADATA_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata_list, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"‚úÖ M√©tadonn√©es enregistr√©es dans '{METADATA_FILE}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'enregistrement du fichier JSON : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "\n",
    "# Fonction pour extraire les couleurs dominantes\n",
    "def get_dominant_colors(image_path, k=3):\n",
    "    \"\"\"\n",
    "    Extrait les k couleurs dominantes d'une image en utilisant K-Means.\n",
    "    :param image_path: Chemin de l'image\n",
    "    :param k: Nombre de couleurs dominantes √† extraire\n",
    "    :return: Liste des couleurs dominantes au format RGB\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"‚ö†Ô∏è Impossible de lire l'image : {image_path}\")\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertir en RGB\n",
    "    pixels = image.reshape(-1, 3)  # Redimensionner en tableau 2D (N pixels x 3 canaux)\n",
    "    kmeans = KMeans(n_clusters=k)  # Appliquer K-Means\n",
    "    kmeans.fit(pixels)\n",
    "    colors = kmeans.cluster_centers_.astype(int)  # Extraire les couleurs dominantes\n",
    "    return colors.tolist()  # Convertir en liste pour JSON\n",
    "\n",
    "# Dossier contenant les images\n",
    "IMAGE_DIR = \"images\"\n",
    "\n",
    "# V√©rifier que le dossier existe\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    print(f\"‚ö†Ô∏è Dossier '{IMAGE_DIR}' introuvable.\")\n",
    "    exit(1)\n",
    "\n",
    "# Dictionnaire pour stocker les couleurs dominantes de chaque image\n",
    "dominant_colors_data = {}\n",
    "\n",
    "# Parcourir toutes les images du dossier\n",
    "for image_name in os.listdir(IMAGE_DIR):\n",
    "    if image_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Filtrer les fichiers images\n",
    "        image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "        print(f\"üîç Traitement de l'image : {image_name}\")\n",
    "\n",
    "        # Extraire les couleurs dominantes\n",
    "        dominant_colors = get_dominant_colors(image_path, k=3)  # Extraire 3 couleurs dominantes\n",
    "        if dominant_colors:\n",
    "            dominant_colors_data[image_name] = dominant_colors\n",
    "            print(f\"‚úÖ Couleurs dominantes pour {image_name} : {dominant_colors}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Aucune couleur dominante trouv√©e pour {image_name}\")\n",
    "\n",
    "# Enregistrer les r√©sultats dans un fichier JSON\n",
    "output_file = os.path.join(IMAGE_DIR, \"dominant_colors.json\")\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dominant_colors_data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Couleurs dominantes enregistr√©es dans '{output_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'enregistrement du fichier JSON : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "# Charger le mod√®le ResNet50 pr√©-entra√Æn√©\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "def classify_image(image_path):\n",
    "    try:\n",
    "        # Charger et pr√©traiter l'image\n",
    "        img = image.load_img(image_path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Pr√©dire les classes\n",
    "        preds = model.predict(x)\n",
    "        return decode_predictions(preds, top=3)[0]\n",
    "\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Erreur : Le fichier {image_path} n'est pas une image valide.\")\n",
    "        return []\n",
    "\n",
    "# Liste pour stocker les pr√©dictions de toutes les images\n",
    "predictions_list = []\n",
    "\n",
    "# Parcourir toutes les images dans le dossier 'images'\n",
    "for file_name in os.listdir('images'):\n",
    "    if file_name.startswith('image_') and file_name.endswith('.jpg'):\n",
    "        image_path = os.path.join('images', file_name)\n",
    "\n",
    "        # Classifier l'image\n",
    "        predictions = classify_image(image_path)\n",
    "\n",
    "        # Ajouter les pr√©dictions √† la liste si l'image est valide\n",
    "        if predictions:\n",
    "            predictions_list.append({\n",
    "                'file_name': file_name,\n",
    "                'predictions': [{'label': pred[1], 'score': float(pred[2])} for pred in predictions]\n",
    "            })\n",
    "\n",
    "        print(f\"Pr√©dictions pour {file_name}: {predictions}\")\n",
    "\n",
    "# Enregistrer toutes les pr√©dictions dans un fichier JSON\n",
    "with open('images/predictions.json', 'w') as f:\n",
    "    json.dump(predictions_list, f, indent=4)\n",
    "\n",
    "print(\"Pr√©dictions enregistr√©es dans 'images/predictions.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
